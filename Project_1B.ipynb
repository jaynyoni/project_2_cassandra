{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "#### This etl project collects and presents user activity information from a streaming service called Sparkify\n",
    "\n",
    "# Part I. ETL Pipeline for Pre-Processing the Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PLEASE RUN THE FOLLOWING CODE FOR PRE-PROCESSING THE FILES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Python packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Python packages \n",
    "import pandas as pd\n",
    "import cassandra\n",
    "import re\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import json\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating list of filepaths to process original event csv data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/jabulaninyoni/Documents/udacity/data_engineering/project_2_cassandra\n"
     ]
    }
   ],
   "source": [
    "# checking your current working directory\n",
    "print(os.getcwd())\n",
    "\n",
    "# Get your current folder and subfolder event data\n",
    "filepath = os.getcwd() + '/event_data'\n",
    "\n",
    "# Create a for loop to create a list of files and collect each filepath\n",
    "for root, dirs, files in os.walk(filepath):\n",
    "    \n",
    "# join the file path and roots with the subdirectories using glob\n",
    "    file_path_list = glob.glob(os.path.join(root,'*'))\n",
    "    #print(file_path_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processing the files to create the data file csv that will be used for Apache Casssandra tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiating an empty list of rows that will be generated from each file\n",
    "full_data_rows_list = [] \n",
    "    \n",
    "# for every filepath in the file path list \n",
    "for f in file_path_list:\n",
    "\n",
    "# reading csv file \n",
    "    with open(f, 'r', encoding = 'utf8', newline='') as csvfile: \n",
    "        # creating a csv reader object \n",
    "        csvreader = csv.reader(csvfile) \n",
    "        next(csvreader)\n",
    "        \n",
    " # extracting each data row one by one and append it        \n",
    "        for line in csvreader:\n",
    "            #print(line)\n",
    "            full_data_rows_list.append(line) \n",
    "            \n",
    "# uncomment the code below if you would like to get total number of rows \n",
    "#print(len(full_data_rows_list))\n",
    "# uncomment the code below if you would like to check to see what the list of event data rows will look like\n",
    "#print(full_data_rows_list)\n",
    "\n",
    "# creating a smaller event data csv file called event_datafile_full csv that will be used to insert data into the \\\n",
    "# Apache Cassandra tables\n",
    "csv.register_dialect('myDialect', quoting=csv.QUOTE_ALL, skipinitialspace=True)\n",
    "\n",
    "with open('event_datafile_new.csv', 'w', encoding = 'utf8', newline='') as f:\n",
    "    writer = csv.writer(f, dialect='myDialect')\n",
    "    writer.writerow(['artist','firstName','gender','itemInSession','lastName','length',\\\n",
    "                'level','location','sessionId','song','userId'])\n",
    "    for row in full_data_rows_list:\n",
    "        if (row[0] == ''):\n",
    "            continue\n",
    "        writer.writerow((row[0], row[2], row[3], row[4], row[5], row[6], row[7], row[8], row[12], row[13], row[16]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6821\n"
     ]
    }
   ],
   "source": [
    "# check the number of rows in your csv file\n",
    "with open('event_datafile_new.csv', 'r', encoding = 'utf8') as f:\n",
    "    print(sum(1 for line in f))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Part II. Creating tables and inserting data into Apache Cassandra"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This should make a connection to a Cassandra instance your local machine \n",
    "# (127.0.0.1)\n",
    "\n",
    "from cassandra.cluster import Cluster\n",
    "cluster = Cluster()\n",
    "\n",
    "# To establish connection and begin executing queries, need a session\n",
    "session = cluster.connect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Keyspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Keyspace created\n",
    "try:\n",
    "    session.execute(\"\"\"\n",
    "    CREATE KEYSPACE IF NOT EXISTS sparkify \n",
    "    WITH REPLICATION = \n",
    "    { 'class' : 'SimpleStrategy', 'replication_factor' : 1 }\"\"\"\n",
    ")\n",
    "\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set Keyspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set KEYSPACE to the keyspace specified above\n",
    "try:\n",
    "    session.set_keyspace('sparkify')\n",
    "except Exception as e:\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below three queries will answer the questions put forward\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query 1\n",
    "\n",
    "this query we want to return artist, song_title and length. We can assume that *session_id* will always be unique. This means that we can partition on this since records of the same session will be stored together. Secondly the query also wants us to sort by *item_in_session* we will add this as an extra clustering key.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating a table that gives the artist, song title and song's length in the music app history that was heard during \\\n",
    "## sessionId = 338, and itemInSession = 4\n",
    "query = \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS artist_song_length(\n",
    "        session_id int,\n",
    "        item_in_session int,\n",
    "        artist_name text,\n",
    "        song_title text,\n",
    "        song_length double,\n",
    "        PRIMARY KEY (session_id, item_in_session)\n",
    "        )\n",
    "        \"\"\"\n",
    "try:\n",
    "    session.execute(query)\n",
    "except Exception as e:\n",
    "     print(e)\n",
    "\n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Insert data into table"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "file = 'event_datafile_new.csv'\n",
    "\n",
    "with open(file, encoding = 'utf8') as f:\n",
    "    csvreader = csv.reader(f)\n",
    "    next(csvreader) # skip header\n",
    "    for line in csvreader:\n",
    "        query = \"INSERT INTO artist_song_length(session_id, item_in_session, artist_name, song_title, song_length)\"\n",
    "        query = query + \"VALUES(%s, %s, %s, %s, %s)\"\n",
    "        artist, firstName, gender, itemInSession, lastName, length, level, location, sessionId, song, userId = line\n",
    "        session.execute(query, (int(sessionId), int(itemInSession), artist, song, float(length)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query table to check data in table is accurate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithless Music Matters (Mark Knight Dub) 495.3073\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "        select artist_name, song_title, song_length from artist_song_length where session_id = 338 \n",
    "        and item_in_session = 4\n",
    "        \"\"\"\n",
    "try:\n",
    "    result = session.execute(query)\n",
    "    for row in result:\n",
    "        print(row.artist_name, row.song_title, row.song_length)\n",
    "except Exception as e:\n",
    "     print(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query 2\n",
    "\n",
    "#####  In this query we want to return name of artist, song (sorted by itemInSession) and user (first and last name) for userid = 10, sessionid = 182 \n",
    "\n",
    "\n",
    "##### To achieve this we are going to use a composite partition key for (user_id and session_id) and a clustering key for item_in_session. We are using a composite paritition key because we want to filter by both (user_id and session_id ). In addition to this we also want to sort by itemInSession that is why we have it as a clustering column.\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a table with name of artist, song (sorted by itemInSession) and user (first and last name)\\\n",
    "## for userid = 10, sessionid = 182\n",
    "query = \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS artist_song_user(\n",
    "        user_id int,\n",
    "        session_id int,\n",
    "        item_in_session int,\n",
    "        artist_name text,\n",
    "        song_title text,\n",
    "        user_first_name text,\n",
    "        user_last_name text,\n",
    "        PRIMARY KEY ((user_id, session_id), item_in_session)\n",
    "        )\n",
    "        \"\"\"\n",
    "try:\n",
    "    session.execute(query)\n",
    "except Exception as e:\n",
    "     print(e)\n",
    "\n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Insert data into table"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with open(file, encoding = 'utf8') as f:\n",
    "    csvreader = csv.reader(f)\n",
    "    next(csvreader) # skip header\n",
    "    for line in csvreader:\n",
    "        query = \"INSERT INTO artist_song_user(user_id, session_id, item_in_session, artist_name, song_title, user_first_name, user_last_name)\"\n",
    "        query = query + \"VALUES(%s, %s, %s, %s, %s, %s, %s)\"\n",
    "        artist, firstName, gender, itemInSession, lastName, length, level, location, sessionId, song, userId = line\n",
    "        session.execute(query, (int(userId),int(sessionId), int(itemInSession), artist, song, firstName, lastName))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Query table to check data in table is accurate"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Down To The Bone Keep On Keepin' On Sylvie Cruz\n",
      "1 Three Drives Greece 2000 Sylvie Cruz\n",
      "2 Sebastien Tellier Kilometer Sylvie Cruz\n",
      "3 Lonnie Gordon Catch You Baby (Steve Pitron & Max Sanna Radio Edit) Sylvie Cruz\n"
     ]
    }
   ],
   "source": [
    "# i added item_in_session in the output so i can see that it is sorted by that column\n",
    "query = \"\"\"\n",
    "        select item_in_session, artist_name, song_title, user_first_name, user_last_name from artist_song_user where user_id = 10 \n",
    "        and session_id = 182\n",
    "        \"\"\"\n",
    "try:\n",
    "    result = session.execute(query)\n",
    "    for row in result:\n",
    "        print(row.item_in_session, row.artist_name, row.song_title, row.user_first_name, row.user_last_name)\n",
    "except Exception as e:\n",
    "     print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query 3\n",
    "\n",
    "#####  In this query we want to return user name (first and last) in my music app history who listened to the song 'All Hands Against His Own' \n",
    "\n",
    "\n",
    "##### To achieve this we are going to use song_title as our partition key since we are filtering on it and the user_id as the clustering key. The assumption is that for each user_id is unique to each user i.e. user first and last name\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a table for every user name (first and last) in my music app history who listened to the song 'All Hands Against His Own'\n",
    "\n",
    "query = \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS user_by_song(\n",
    "        song_title text,\n",
    "        user_id int,\n",
    "        user_first_name text,\n",
    "        user_last_name text,\n",
    "        artist_name text,\n",
    "        song_length double,\n",
    "        PRIMARY KEY (song_title, user_id)\n",
    "        )\n",
    "        \"\"\"\n",
    "try:\n",
    "    session.execute(query)\n",
    "except Exception as e:\n",
    "     print(e)\n",
    "\n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Insert data into table"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "file = 'event_datafile_new.csv'\n",
    "\n",
    "with open(file, encoding = 'utf8') as f:\n",
    "    csvreader = csv.reader(f)\n",
    "    next(csvreader) # skip header\n",
    "    for line in csvreader:\n",
    "        query = \"INSERT INTO user_by_song(song_title, user_id, user_first_name, user_last_name, artist_name, song_length)\"\n",
    "        query = query + \"VALUES(%s, %s, %s, %s, %s, %s)\"\n",
    "        artist, firstName, gender, itemInSession, lastName, length, level, location, sessionId, song, userId = line\n",
    "        session.execute(query, (song, int(userId),firstName, lastName, artist, float(length)))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Query table to check data in table is accurate"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jacqueline Lynch\n",
      "Tegan Levine\n",
      "Sara Johnson\n"
     ]
    }
   ],
   "source": [
    "# every user name (first and last)\n",
    "query = \"\"\"\n",
    "        select user_first_name, user_last_name from user_by_song\n",
    "        where song_title = 'All Hands Against His Own' \n",
    "        \"\"\"\n",
    "try:\n",
    "    result = session.execute(query)\n",
    "    for row in result:\n",
    "        print(row.user_first_name, row.user_last_name)\n",
    "except Exception as e:\n",
    "     print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop the tables before closing out the sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "        DROP TABLE IF EXISTS artist_song_length\n",
    "        \"\"\"\n",
    "try:\n",
    "    session.execute(query)\n",
    "except Exception as e:\n",
    "     print(e)\n",
    "        \n",
    "query = \"\"\"\n",
    "        DROP TABLE IF EXISTS artist_song_user\n",
    "        \"\"\"\n",
    "try:\n",
    "    session.execute(query)\n",
    "except Exception as e:\n",
    "     print(e)\n",
    "        \n",
    "query = \"\"\"\n",
    "        DROP TABLE IF EXISTS user_by_song\n",
    "        \"\"\"\n",
    "try:\n",
    "    session.execute(query)\n",
    "except Exception as e:\n",
    "     print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Close the session and cluster connection¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.shutdown()\n",
    "cluster.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}